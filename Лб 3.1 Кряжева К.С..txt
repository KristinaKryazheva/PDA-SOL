[cloudera@quickstart ~]$ hdfs version
Hadoop 2.6.0-cdh5.13.0
Subversion http://github.com/cloudera/hadoop -r 42e8860b182e55321bd5f5605264da4adc8882be
Compiled by jenkins on 2017-10-04T18:08Z
Compiled with protoc 2.5.0
From source with checksum 5e84c185f8a22158e2b0e4b8f85311
This command was run using /usr/lib/hadoop/hadoop-common-2.6.0-cdh5.13.0.jar


3.1.2. Вывести с помощью команды help описание основных команды shell-клиента.
[cloudera@quickstart ~]$ help
GNU bash, version 4.1.2(1)-release (x86_64-redhat-linux-gnu)
These shell commands are defined internally.  Type `help' to see this list.
Type `help name' to find out more about the function `name'.
Use `info bash' to find out more about the shell in general.
Use `man -k' or `info' to find out more about commands not in this list.

A star (*) next to a name means that the command is disabled.

 job_spec [&]                            history [-c] [-d offset] [n] or hist>
 (( expression ))                        if COMMANDS; then COMMANDS; [ elif C>
 . filename [arguments]                  jobs [-lnprs] [jobspec ...] or jobs >
 :                                       kill [-s sigspec | -n signum | -sigs>
 [ arg... ]                              let arg [arg ...]
 [[ expression ]]                        local [option] name[=value] ...
 alias [-p] [name[=value] ... ]          logout [n]
 bg [job_spec ...]                       mapfile [-n count] [-O origin] [-s c>
 bind [-lpvsPVS] [-m keymap] [-f filen>  popd [-n] [+N | -N]
 break [n]                               printf [-v var] format [arguments]
 builtin [shell-builtin [arg ...]]       pushd [-n] [+N | -N | dir]
 caller [expr]                           pwd [-LP]
 case WORD in [PATTERN [| PATTERN]...)>  read [-ers] [-a array] [-d delim] [->
 cd [-L|-P] [dir]                        readarray [-n count] [-O origin] [-s>
 command [-pVv] command [arg ...]        readonly [-af] [name[=value] ...] or>
 compgen [-abcdefgjksuv] [-o option]  >  return [n]
 complete [-abcdefgjksuv] [-pr] [-DE] >  select NAME [in WORDS ... ;] do COMM>
 compopt [-o|+o option] [-DE] [name ..>  set [--abefhkmnptuvxBCHP] [-o option>
 continue [n]                            shift [n]
 coproc [NAME] command [redirections]    shopt [-pqsu] [-o] [optname ...]
 declare [-aAfFilrtux] [-p] [name[=val>  source filename [arguments]
 dirs [-clpv] [+N] [-N]                  suspend [-f]
 disown [-h] [-ar] [jobspec ...]         test [expr]
 echo [-neE] [arg ...]                   time [-p] pipeline
 enable [-a] [-dnps] [-f filename] [na>  times
 eval [arg ...]                          trap [-lp] [[arg] signal_spec ...]
 exec [-cl] [-a name] [command [argume>  true
 exit [n]                                type [-afptP] name [name ...]
 export [-fn] [name[=value] ...] or ex>  typeset [-aAfFilrtux] [-p] name[=val>
 false                                   ulimit [-SHacdefilmnpqrstuvx] [limit>
 fc [-e ename] [-lnr] [first] [last] o>  umask [-p] [-S] [mode]
 fg [job_spec]                           unalias [-a] name [name ...]
 for NAME [in WORDS ... ] ; do COMMAND>  unset [-f] [-v] [name ...]
 for (( exp1; exp2; exp3 )); do COMMAN>  until COMMANDS; do COMMANDS; done
 function name { COMMANDS ; } or name >  variables - Names and meanings of so>
 getopts optstring name [arg]            wait [id]
 hash [-lr] [-p pathname] [-dt] [name >  while COMMANDS; do COMMANDS; done
 help [-dms] [pattern ...]               { COMMANDS ; }
[cloudera@quickstart ~]$ 



3.1.3. Просмотреть корневую директорию HDFS.
[cloudera@quickstart ~]$ hdfs dfs -ls /
Found 6 items
drwxrwxrwx   - hdfs  supergroup          0 2017-10-23 09:15 /benchmarks
drwxr-xr-x   - hbase supergroup          0 2024-03-22 10:32 /hbase
drwxr-xr-x   - solr  solr                0 2017-10-23 09:18 /solr
drwxrwxrwt   - hdfs  supergroup          0 2024-02-17 00:38 /tmp
drwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /user
drwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /var
[cloudera@quickstart ~]$ 


3.1.4. Создать в HDFS в директории /user/mgpu поддиректорию ваше_фио.
cloudera@quickstart ~]$ hdfs dfs -mkdir /user/mgpu
[cloudera@quickstart ~]$ hdfs dfs -mkdir /user/mgpu/KryazhevaKS
[cloudera@quickstart ~]$ base64 /dev/urandom | head -c 10000000 > Kryazheva.txt
[cloudera@quickstart ~]$ 


3.1.5. Создать в локальной файловой системе случайный текстовый файл размером 10 Mb с именем, образованным вашими инициалами base64 /dev/urandom | head -c 10000000 > file.txt .

cloudera@quickstart ~]$ ls
cloudera-manager  enterprise-deployment.json  lib       Templates
cm_api.py         express-deployment.json     __MACOSX  trucks.csv
Desktop           geolocation.csv             Music     Videos
Documents         geolocation.zip             parcels   workspace
Downloads         kerberos                    Pictures
eclipse           Kryazheva.txt               Public
[cloudera@quickstart ~]$ 


3.1.6. Заархивировать созданный текстовый файл gzip -c file.txt > file.gz .

[cloudera@quickstart ~]$ gzip -c Kryazheva.txt > Kryazheva.gz
[cloudera@quickstart ~]$ ls
cloudera-manager  enterprise-deployment.json  Kryazheva.txt  Public
cm_api.py         express-deployment.json     lib            Templates
Desktop           geolocation.csv             __MACOSX       trucks.csv
Documents         geolocation.zip             Music          Videos
Downloads         kerberos                    parcels        workspace
eclipse           Kryazheva.gz  

3.1.7. Скопировать текстовый файл и архив в директорию /user/mgpu/fio HDFS виртуальной машины.

[cloudera@quickstart ~]$ hdfs dfs -copyFromLocal ./Kryazheva.* /user/mgpu/KryazhevaKS


3.1.8. Просмотреть файл и архив с помощью утилит cat, text в комбинации с каналами и утилитами head, tail -- привести не менее 3 вариантов команд и просмотра файла.

[cloudera@quickstart ~]$ hdfs dfs -cat /user/mgpu/KryazhevaKS/Kryazheva.txt | head -n 3
KJ5AOGBCYrLIDZTNBO+vYX1+nOCLkSgCrvFE0Bvuf+8tcwgUbOOXToOZUfSmrGufXSdnob4XED6j
wwMSeJFxwXWsHdgCEgp/oTGnI1V6BD1I9JtVTx4kzgpBYGVXWZegXyaqUlKutBii1OPAJ8kw0Vp8
ImaBG6z2jeTOTGTK1+GWlJ7nQrzK8LWJv99FWfoVHa4hwl6PLUicrBQbBe7iMTQj7fpm1YONYZCp
cat: Unable to write to output stream.

[cloudera@quickstart ~]$ hdfs dfs -cat /user/mgpu/KryazhevaKS/Kryazheva.txt | tail -n 3

p3187l+WpX4uMu0Bx2P9CEMvyzDJgAf64O5X1iFVtOCWrah7LHe6IaG5fErNLWHnx9yggj8Q+t9j
KXDF0Mef155hOeomQUfIHLMIcRwHiQmnqDeROACEELGdPUkuqArU3BWselemXYEqEZUUtrlqDVtO
0tkJGWCaBn[cloudera@quickstart ~]$ 

[cloudera@quickstart ~]$ hdfs dfs -cat /user/mgpu/KryazhevaKS/Kryazheva.gz | gunzip | head -n 3

KJ5AOGBCYrLIDZTNBO+vYX1+nOCLkSgCrvFE0Bvuf+8tcwgUbOOXToOZUfSmrGufXSdnob4XED6j
wwMSeJFxwXWsHdgCEgp/oTGnI1V6BD1I9JtVTx4kzgpBYGVXWZegXyaqUlKutBii1OPAJ8kw0Vp8
ImaBG6z2jeTOTGTK1+GWlJ7nQrzK8LWJv99FWfoVHa4hwl6PLUicrBQbBe7iMTQj7fpm1YONYZCp
cat: Unable to write to output stream.




3.1.9. Создать копию файла file.txt вида date_file.txt, где в начале имени файла-копии указана текущая дата. Вывести листинг.

cloudera@quickstart ~]$ hdfs dfs -cp /user/mgpu/KryazhevaKS/Kryazheva.txt /user/mgpu/KryazhevaKS/22march.txt


3.1.10. Вывести статистику по директории /user/mgpu/fio виртуальной машины.

[cloudera@quickstart ~]$ hdfs dfs -ls /user/mgpu/KryazhevaKS

Found 3 items
-rw-r--r--   1 cloudera supergroup   10000000 2024-03-22 10:57 /user/mgpu/KryazhevaKS/22march.txt
-rw-r--r--   1 cloudera supergroup    7599429 2024-03-22 10:50 /user/mgpu/KryazhevaKS/Kryazheva.gz
-rw-r--r--   1 cloudera supergroup   10000000 2024-03-22 10:50 /user/mgpu/KryazhevaKS/Kryazheva.txt

3.1.11. Удалить поддиректорию /fio со всем содержимым.
[cloudera@quickstart ~]$ hdfs dfs -rm -r -f /user/mgpu/KryazhevaKS
Deleted /user/mgpu/KryazhevaKS


3.1.12. Подсчитать количество слов в файле внутри HDFS с помощью методологии Map Reduce (размер файла не менее 128 Мб).


[cloudera@quickstart ~]$ yarn jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar pi 4 100
Number of Maps  = 4
Samples per Map = 100
Wrote input for Map #0
Wrote input for Map #1
Wrote input for Map #2
Wrote input for Map #3
Starting Job
24/04/05 02:48:30 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
24/04/05 02:48:30 INFO input.FileInputFormat: Total input paths to process : 4
24/04/05 02:48:31 INFO mapreduce.JobSubmitter: number of splits:4
24/04/05 02:48:31 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1712307884597_0001
24/04/05 02:48:32 INFO impl.YarnClientImpl: Submitted application application_1712307884597_0001
24/04/05 02:48:32 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1712307884597_0001/
24/04/05 02:48:32 INFO mapreduce.Job: Running job: job_1712307884597_0001
24/04/05 02:48:44 INFO mapreduce.Job: Job job_1712307884597_0001 running in uber mode : false
24/04/05 02:48:44 INFO mapreduce.Job:  map 0% reduce 0%
24/04/05 02:50:10 INFO mapreduce.Job:  map 25% reduce 0%
24/04/05 02:50:21 INFO mapreduce.Job:  map 75% reduce 0%
24/04/05 02:50:23 INFO mapreduce.Job:  map 100% reduce 0%
24/04/05 02:51:02 INFO mapreduce.Job:  map 100% reduce 100%
24/04/05 02:51:03 INFO mapreduce.Job: Job job_1712307884597_0001 completed successfully
24/04/05 02:51:04 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=94
		FILE: Number of bytes written=719986
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1112
		HDFS: Number of bytes written=215
		HDFS: Number of read operations=19
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Job Counters 
		Launched map tasks=4
		Launched reduce tasks=1
		Data-local map tasks=4
		Total time spent by all maps in occupied slots (ms)=360610
		Total time spent by all reduces in occupied slots (ms)=45646
		Total time spent by all map tasks (ms)=360610
		Total time spent by all reduce tasks (ms)=45646
		Total vcore-milliseconds taken by all map tasks=360610
		Total vcore-milliseconds taken by all reduce tasks=45646
		Total megabyte-milliseconds taken by all map tasks=369264640
		Total megabyte-milliseconds taken by all reduce tasks=46741504
	Map-Reduce Framework
		Map input records=4
		Map output records=8
		Map output bytes=72
		Map output materialized bytes=112
		Input split bytes=640
		Combine input records=0
		Combine output records=0
		Reduce input groups=2
		Reduce shuffle bytes=112
		Reduce input records=8
		Reduce output records=0
		Spilled Records=16
		Shuffled Maps =4
		Failed Shuffles=0
		Merged Map outputs=4
		GC time elapsed (ms)=7772
		CPU time spent (ms)=35310
		Physical memory (bytes) snapshot=1300389888
		Virtual memory (bytes) snapshot=7792926720
		Total committed heap usage (bytes)=1195900928
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=472
	File Output Format Counters 
		Bytes Written=97
Job Finished in 154.182 seconds
Estimated value of Pi is 3.17000000000000000000





[cloudera@quickstart ~]$ hdfs dfs -ls /
Found 6 items
drwxrwxrwx   - hdfs  supergroup          0 2017-10-23 09:15 /benchmarks
drwxr-xr-x   - hbase supergroup          0 2024-04-05 02:05 /hbase
drwxr-xr-x   - solr  solr                0 2017-10-23 09:18 /solr
drwxrwxrwt   - hdfs  supergroup          0 2024-02-17 00:38 /tmp
drwxr-xr-x   - hdfs  supergroup          0 2024-03-22 10:42 /user
drwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /var
[cloudera@quickstart ~]$ hdfs dfs -mv data geoloc




cloudera@quickstart ~]$ yarn jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar wordcount geoloc/geolocation.csv output
24/04/05 03:01:10 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032
24/04/05 03:01:11 INFO input.FileInputFormat: Total input paths to process : 1
24/04/05 03:01:11 INFO mapreduce.JobSubmitter: number of splits:1
24/04/05 03:01:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1712307884597_0003
24/04/05 03:01:12 INFO impl.YarnClientImpl: Submitted application application_1712307884597_0003
24/04/05 03:01:12 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1712307884597_0003/
24/04/05 03:01:12 INFO mapreduce.Job: Running job: job_1712307884597_0003
24/04/05 03:01:19 INFO mapreduce.Job: Job job_1712307884597_0003 running in uber mode : false
24/04/05 03:01:19 INFO mapreduce.Job:  map 0% reduce 0%
24/04/05 03:01:25 INFO mapreduce.Job:  map 100% reduce 0%
24/04/05 03:02:21 INFO mapreduce.Job:  map 100% reduce 100%
24/04/05 03:02:24 INFO mapreduce.Job: Job job_1712307884597_0003 completed successfully
24/04/05 03:02:24 INFO mapreduce.Job: Counters: 49
	File System Counters
		FILE: Number of bytes read=331421
		FILE: Number of bytes written=950105
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=526810
		HDFS: Number of bytes written=308844
		HDFS: Number of read operations=6
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=2
	Job Counters 
		Launched map tasks=1
		Launched reduce tasks=1
		Data-local map tasks=1
		Total time spent by all maps in occupied slots (ms)=4413
		Total time spent by all reduces in occupied slots (ms)=3420
		Total time spent by all map tasks (ms)=4413
		Total time spent by all reduce tasks (ms)=3420
		Total vcore-milliseconds taken by all map tasks=4413
		Total vcore-milliseconds taken by all reduce tasks=3420
		Total megabyte-milliseconds taken by all map tasks=4518912
		Total megabyte-milliseconds taken by all reduce tasks=3502080
	Map-Reduce Framework
		Map input records=8001
		Map output records=12110
		Map output bytes=567116
		Map output materialized bytes=331421
		Input split bytes=133
		Combine input records=12110
		Combine output records=5681
		Reduce input groups=5681
		Reduce shuffle bytes=331421
		Reduce input records=5681
		Reduce output records=5681
		Spilled Records=11362
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=88
		CPU time spent (ms)=2800
		Physical memory (bytes) snapshot=526606336
		Virtual memory (bytes) snapshot=3133554688
		Total committed heap usage (bytes)=414711808
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=526677
	File Output Format Counters 
		Bytes Written=308844



[cloudera@quickstart ~]$ hdfs dfs -ls output
Found 2 items
-rw-r--r--   1 cloudera cloudera          0 2024-04-05 03:01 output/_SUCCESS
-rw-r--r--   1 cloudera cloudera     308844 2024-04-05 03:01 output/part-r-00000



